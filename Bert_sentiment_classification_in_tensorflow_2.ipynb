{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "c9RcwUhZGBT4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7y2_Vak8LN8",
        "outputId": "b0b940cc-0342-43b2-d87f-8358cc02e425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        " # https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz\n",
        "! pip install bert-for-tf2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/c1/015648a2186b25c6de79d15bec40d3d946fcf1dd5067d1c1b28009506486/bert-for-tf2-0.14.6.tar.gz (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 26.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 30kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 2.4MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.6-cp36-none-any.whl size=30318 sha256=e75dc003881deff388dac3c4449328e5653768ce86a372b4b87c5df52876511a\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/a0/b4/75b0601ebaa41e517a797fe9cea119c789664c8408f8a74ae9\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7304 sha256=6c4f3b1a804c5d026af82986e484a360b128571ce0dc69eba9f9c5519dda4bcc\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19475 sha256=f3a224655cb97d4a980922a1747d7171f6dc9594057f4ed87f4bf6894cba51f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.6 params-flow-0.8.2 py-params-0.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdwLsC7K_NzV"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "import tarfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mp\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import keras\n",
        "# imports for bert\n",
        "import bert\n",
        "from bert import BertModelLayer\n",
        "from bert.loader import StockBertConfig,map_stock_config_to_params,load_stock_weights\n",
        "from bert.tokenization.bert_tokenization import FullTokenizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJlYZs409IF9"
      },
      "source": [
        "url = 'https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('text.tgz', 'wb').write(r.content)\n",
        "\n",
        "my_tar = tarfile.open('/content/text.tgz')\n",
        "my_tar.extractall('/content') # specify which folder to extract to\n",
        "my_tar.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myY-VzZwB_Mv",
        "outputId": "81409135-9dff-40c9-9421-fef6950e8032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "f = open(\"/content/yelp_review_polarity_csv/readme.txt\", \"r\")\n",
        "print(f.read())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Yelp Review Polarity Dataset\n",
            "\n",
            "Version 1, Updated 09/09/2015\n",
            "\n",
            "ORIGIN\n",
            "\n",
            "The Yelp reviews dataset consists of reviews from Yelp. It is extracted from the Yelp Dataset Challenge 2015 data. For more information, please refer to http://www.yelp.com/dataset_challenge\n",
            "\n",
            "The Yelp reviews polarity dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the above dataset. It is first used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n",
            "\n",
            "\n",
            "DESCRIPTION\n",
            "\n",
            "The Yelp reviews polarity dataset is constructed by considering stars 1 and 2 negative, and 3 and 4 positive. For each polarity 280,000 training samples and 19,000 testing samples are take randomly. In total there are 560,000 trainig samples and 38,000 testing samples. Negative polarity is class 1, and positive class 2.\n",
            "\n",
            "The files train.csv and test.csv contain all the training samples as comma-sparated values. There are 2 columns in them, corresponding to class index (1 and 2) and review text. The review texts are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). New lines are escaped by a backslash followed with an \"n\" character, that is \"\\n\".\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXgAozgzCSG1"
      },
      "source": [
        "train = pd.read_csv('/content/yelp_review_polarity_csv/train.csv',header=None,names=['intent','text'])\n",
        "test = pd.read_csv('/content/yelp_review_polarity_csv/test.csv',header=None,names=['intent','text'])\n",
        "\n",
        "train = pd.DataFrame(\n",
        "    {'text' : train['text'],\n",
        "    'intent' : train['intent']})\n",
        "\n",
        "test = pd.DataFrame(\n",
        "    {'text' : test['text'],\n",
        "    'intent' : test['intent']})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRsaoFD1Tlhk",
        "outputId": "b1c2c59d-b2e3-4460-c039-e30f40ef91e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-24 16:20:37--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.195.128, 74.125.142.128, 74.125.20.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.195.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   125MB/s    in 3.1s    \n",
            "\n",
            "2020-09-24 16:20:40 (125 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMJ2dTxqTqhX"
      },
      "source": [
        "tokenizer = FullTokenizer(\n",
        "  vocab_file=os.path.join('/content/uncased_L-12_H-768_A-12/vocab.txt')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1fj3pcDdM8n",
        "outputId": "b4b7a786-00e5-471e-ac57-2b58b04d9412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.tokenize(\"I can't wait to exercise!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'can', \"'\", 't', 'wait', 'to', 'exercise', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8oLpuPXTqPK",
        "outputId": "9b62679a-c885-4cd7-8ff1-6787e6dc0412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokens=tokenizer.tokenize(\"I can't wait wait to exercise!\")\n",
        "tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1045, 2064, 1005, 1056, 3524, 3524, 2000, 6912, 999]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giWCGmxodQzG"
      },
      "source": [
        "bert_config_file = os.path.join('/content/uncased_L-12_H-768_A-12/bert_config.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZriONGQeyoo"
      },
      "source": [
        "# preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQPFA1qGxTb9"
      },
      "source": [
        "def preprocess_input(x):\n",
        "  k=tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokenizer.tokenize(x['text'])+[\"[SEP]\"])\n",
        "  if len(k)<200 : k = k[:200]\n",
        "  else: k = k + [0]*(200-len(k))\n",
        "  return k                              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o98fh-pwYWej"
      },
      "source": [
        "def preprocess_input_2(x):\n",
        "  k=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x['text']))\n",
        "  if len(k)>198 : k = k[:198]\n",
        "\n",
        "  k=tokenizer.convert_tokens_to_ids([\"[CLS]\"])+k+tokenizer.convert_tokens_to_ids([\"[SEP]\"])\n",
        "  if len(k)<200:\n",
        "    k= k + [0]*(200-len(k))\n",
        "\n",
        "  return k  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NghAzdrruhNd"
      },
      "source": [
        "#train_1=pd.DataFrame(train.iloc[:1000,:].apply(lambda x: pd.Series([tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokenizer.tokenize(x['text'])+[\"[SEP]\"]),x['intent']], index=['text', 'intent']), axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO5-eBGRvJcS"
      },
      "source": [
        "train_processed=pd.DataFrame(train.iloc[:50000,:].apply(lambda x: pd.Series([preprocess_input_2(x),x['intent']], index=['text', 'intent']), axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-J7yW_CKZhQ"
      },
      "source": [
        "test_processed=pd.DataFrame(test.iloc[:50,:].apply(lambda x: pd.Series([preprocess_input_2(x),x['intent']], index=['text', 'intent']), axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvOus1ruXZuR",
        "outputId": "68b32565-bba2-4eb7-e59c-2adeb0f03887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(test_processed.iloc[0,0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRZMIAGPJEwL"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVqK6g6-1_LM"
      },
      "source": [
        "with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
        "      bc = StockBertConfig.from_json_string(reader.read())\n",
        "      bert_params = map_stock_config_to_params(bc)\n",
        "      bert_params.adapter_size = None\n",
        "      bert = BertModelLayer.from_params(bert_params, name=\"bert\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eErXsU12wkN"
      },
      "source": [
        "input_ids = keras.layers.Input(\n",
        "    shape=(200, ),\n",
        "    dtype='int32',\n",
        "    name=\"input_ids\"\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_W1d41s28I_"
      },
      "source": [
        "bert_output = bert(input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6dRcmzc6xcI",
        "outputId": "43b49683-91e2-4709-e04f-a3a407ad62d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"bert shape\", bert_output.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert shape (None, 200, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdXdGHrl61Wo"
      },
      "source": [
        "cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
        "cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
        "logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
        "logits = keras.layers.Dropout(0.5)(logits)\n",
        "logits = keras.layers.Dense(units=2,activation=\"softmax\")(logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vDbppFI7QjH"
      },
      "source": [
        "model = keras.Model(inputs=input_ids, outputs=logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rONbiI9_7ZJn"
      },
      "source": [
        "model.build(input_shape=(None, 200))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7eufCOf8HYD"
      },
      "source": [
        "# 1e-5\n",
        "model.compile(\n",
        "  optimizer=keras.optimizers.Adam(1e-5),\n",
        "  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9pWfjLM8JOm",
        "outputId": "ddd8be34-db4e-4d16-e69f-dd36eb96b0d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "model.fit(\n",
        "  x=np.array(list(pd.Series(train_processed['text'],index=None))),\n",
        "  y=train_processed['intent'].to_numpy()-1,\n",
        "  validation_split=0.02,\n",
        "  batch_size=16,\n",
        "  shuffle=True,\n",
        "  epochs=3\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "3063/3063 [==============================] - 2784s 909ms/step - loss: 0.5667 - acc: 0.7305 - val_loss: 0.4451 - val_acc: 0.8600\n",
            "Epoch 2/3\n",
            "3063/3063 [==============================] - 2776s 906ms/step - loss: 0.4497 - acc: 0.8597 - val_loss: 0.4308 - val_acc: 0.8780\n",
            "Epoch 3/3\n",
            "3063/3063 [==============================] - 2778s 907ms/step - loss: 0.4293 - acc: 0.8807 - val_loss: 0.4247 - val_acc: 0.8810\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc1e3f177f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKIEQyL6JLIV"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDBgztz5JPDm",
        "outputId": "225321db-9ffb-4bb3-ffad-cab9e55f8e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "_, train_acc = model.evaluate(np.array(list(pd.Series(train_processed['text'],index=None))), train_processed['intent'].to_numpy()-1)\n",
        "_, test_acc =  model.evaluate(np.array(list(pd.Series(test_processed['text'],index=None))), test_processed['intent'].to_numpy()-1)\n",
        "print(\"train acc\", train_acc)\n",
        "print(\"test acc\", test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 741s 474ms/step - loss: 0.4095 - acc: 0.9013\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.3582 - acc: 0.9600\n",
            "train acc 0.9012600183486938\n",
            "test acc 0.9599999785423279\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9RcwUhZGBT4"
      },
      "source": [
        "# Rough - dump"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAvYqYNA8JFW"
      },
      "source": [
        "'''class IntentDetectionData:\n",
        "  DATA_COLUMN = \"text\"\n",
        "  LABEL_COLUMN = \"intent\"\n",
        "  def __init__(\n",
        "    self,\n",
        "    train,\n",
        "    test,\n",
        "    tokenizer: FullTokenizer,\n",
        "    classes,\n",
        "    max_seq_len=192\n",
        "  ):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_seq_len = 0\n",
        "    self.classes = classes\n",
        "    ((self.train_x, self.train_y), (self.test_x, self.test_y)) =\\\n",
        "     map(self._prepare, [train, test])\n",
        "    print(\"max seq_len\", self.max_seq_len)\n",
        "    self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
        "    self.train_x, self.test_x = map(\n",
        "      self._pad,\n",
        "      [self.train_x, self.test_x]\n",
        "    )\n",
        "  def _prepare(self, df):\n",
        "    x, y = [], []\n",
        "    for _, row in tqdm(df.iterrows()):\n",
        "      text, label =\\\n",
        "       row[IntentDetectionData.DATA_COLUMN], \\\n",
        "       row[IntentDetectionData.LABEL_COLUMN]\n",
        "      tokens = self.tokenizer.tokenize(text)\n",
        "      tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "      token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "      self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
        "      x.append(token_ids)\n",
        "      y.append(self.classes.index(label))\n",
        "    return np.array(x), np.array(y)\n",
        "  def _pad(self, ids):\n",
        "    x = []\n",
        "    for input_ids in ids:\n",
        "      input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
        "      input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
        "      x.append(np.array(input_ids))\n",
        "    return np.array(x)\n",
        "    \n",
        "    \n",
        "  classes = train.intent.unique().tolist()\n",
        "data = IntentDetectionData(\n",
        "  train.iloc[:100,:],\n",
        "  test.iloc[:100,:],\n",
        "  tokenizer,\n",
        "  classes,\n",
        "  max_seq_len=128\n",
        ")  \n",
        " '''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}